"""
Airflow DAG: data_validation_pipeline
- ExÃ©cuter la validation de qualitÃ© des donnÃ©es (complÃ©tude, cohÃ©rence, formats)
- AprÃ¨s validation rÃ©ussie: lire les donnÃ©es transformÃ©es depuis MongoDB staging
- Sauvegarder les donnÃ©es validÃ©es dans la collection MongoDB core_data
- Logger toutes les Ã©tapes dans Airflow (succÃ¨s, erreurs, nombre de lignes)
- Envoyer alertes si la validation Ã©choue ou si la sauvegarde MongoDB Ã©choue
"""
from __future__ import annotations

import json
from datetime import datetime, timedelta
from typing import Dict, Any

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.exceptions import AirflowFailException

from config.settings import config
from youtube_elt.db import get_collection
from youtube_elt.load import transform_and_upsert_core


DEFAULT_ARGS = {
    "owner": "data-engineering",
    "depends_on_past": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}


def validate_data_quality(**context) -> Dict[str, Any]:
    """
    Valider la qualitÃ© des donnÃ©es dans la collection staging.
    
    Returns:
        Dict contenant les rÃ©sultats de validation
    """
    print("ğŸ” === DÃ‰BUT DE LA VALIDATION DE QUALITÃ‰ DES DONNÃ‰ES ===")
    
    try:
        staging_collection = get_collection(config.STAGING_COLLECTION)
        
        # Compter les enregistrements
        total_records = staging_collection.count_documents({})
        print(f"ğŸ“Š Total des enregistrements staging: {total_records}")
        
        if total_records == 0:
            raise AirflowFailException("âŒ Ã‰CHEC: Aucun enregistrement trouvÃ© dans staging")
        
        # Tests de complÃ©tude
        print("\nğŸ” Tests de complÃ©tude:")
        required_fields = ['video_id', 'title', 'view_count', 'like_count']
        completeness_issues = []
        
        for field in required_fields:
            null_count = staging_collection.count_documents({field: None})
            empty_count = staging_collection.count_documents({field: ""})
            total_issues = null_count + empty_count
            
            if total_issues > 0:
                issue = f"{field}: {total_issues} valeurs manquantes"
                completeness_issues.append(issue)
                print(f"   âŒ {issue}")
            else:
                print(f"   âœ… {field}: Complet ({total_records} valeurs)")
        
        # Tests de cohÃ©rence
        print("\nğŸ” Tests de cohÃ©rence:")
        consistency_issues = []
        
        # VÃ©rifier view_count positif
        negative_views = staging_collection.count_documents({"view_count": {"$lt": 0}})
        if negative_views > 0:
            issue = f"view_count nÃ©gatif: {negative_views} enregistrements"
            consistency_issues.append(issue)
            print(f"   âŒ {issue}")
        else:
            print("   âœ… view_count: Tous positifs")
        
        # VÃ©rifier like_count positif
        negative_likes = staging_collection.count_documents({"like_count": {"$lt": 0}})
        if negative_likes > 0:
            issue = f"like_count nÃ©gatif: {negative_likes} enregistrements"
            consistency_issues.append(issue)
            print(f"   âŒ {issue}")
        else:
            print("   âœ… like_count: Tous positifs")
        
        # RÃ©sumÃ© de validation
        total_issues = len(completeness_issues) + len(consistency_issues)
        
        validation_result = {
            "status": "PASSED" if total_issues == 0 else "FAILED",
            "total_records": total_records,
            "completeness_issues": completeness_issues,
            "consistency_issues": consistency_issues,
            "total_issues": total_issues,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        if total_issues == 0:
            print(f"\nğŸ‰ VALIDATION RÃ‰USSIE: {total_records} enregistrements validÃ©s")
        else:
            print(f"\nâš ï¸ VALIDATION Ã‰CHOUÃ‰E: {total_issues} problÃ¨mes dÃ©tectÃ©s")
            for issue in completeness_issues + consistency_issues:
                print(f"   - {issue}")
            
            # Lever une exception pour arrÃªter le pipeline
            raise AirflowFailException(f"Validation Ã©chouÃ©e: {total_issues} problÃ¨mes dÃ©tectÃ©s")
        
        # Stocker les rÃ©sultats dans XCom
        context["ti"].xcom_push(key="validation_result", value=validation_result)
        
        print(f"ğŸ“Š Statut de validation: {validation_result['status']}")
        return validation_result
        
    except AirflowFailException:
        raise
    except Exception as e:
        error_msg = f"Erreur lors de la validation: {str(e)}"
        print(f"âŒ {error_msg}")
        raise AirflowFailException(error_msg)


def read_and_transform_staging_data(**context) -> Dict[str, Any]:
    """
    Lire les donnÃ©es validÃ©es depuis staging et les transformer.
    
    Returns:
        Dict contenant les statistiques de transformation
    """
    print("ğŸ”„ === LECTURE ET TRANSFORMATION DES DONNÃ‰ES STAGING ===")
    
    try:
        # VÃ©rifier que la validation a rÃ©ussi
        validation_result = context["ti"].xcom_pull(key="validation_result", task_ids="validate_quality")
        
        if not validation_result or validation_result.get("status") != "PASSED":
            raise AirflowFailException("âŒ Validation prÃ©alable Ã©chouÃ©e, arrÃªt de la transformation")
        
        print(f"âœ… Validation confirmÃ©e: {validation_result['total_records']} enregistrements validÃ©s")
        
        # Lire les donnÃ©es de staging
        staging_collection = get_collection(config.STAGING_COLLECTION)
        staging_records = list(staging_collection.find({}, {"_id": 0}))
        
        print(f"ğŸ“– Lecture de {len(staging_records)} enregistrements depuis staging")
        
        if not staging_records:
            raise AirflowFailException("âŒ Aucune donnÃ©e trouvÃ©e dans staging")
        
        # CrÃ©er le dataset pour transformation
        dataset = {
            "channel_handle": config.YOUTUBE_CHANNEL_HANDLE,
            "extraction_date": datetime.utcnow().isoformat(),
            "total_videos": len(staging_records),
            "videos": staging_records
        }
        
        print(f"âœ… Dataset crÃ©Ã© avec {len(staging_records)} vidÃ©os")
        
        # Stocker dans XCom pour la tÃ¢che suivante
        context["ti"].xcom_push(key="transformed_dataset", value=dataset)
        
        transform_result = {
            "records_read": len(staging_records),
            "dataset_created": True,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        print(f"ğŸ“Š Transformation prÃ©parÃ©e: {transform_result['records_read']} enregistrements")
        return transform_result
        
    except AirflowFailException:
        raise
    except Exception as e:
        error_msg = f"Erreur lors de la lecture/transformation: {str(e)}"
        print(f"âŒ {error_msg}")
        raise AirflowFailException(error_msg)


def save_validated_data_to_core(**context) -> Dict[str, Any]:
    """
    Sauvegarder les donnÃ©es validÃ©es dans la collection core_data.
    
    Returns:
        Dict contenant les statistiques de sauvegarde
    """
    print("ğŸ’¾ === SAUVEGARDE DES DONNÃ‰ES VALIDÃ‰ES DANS CORE ===")
    
    try:
        # RÃ©cupÃ©rer le dataset transformÃ©
        dataset = context["ti"].xcom_pull(key="transformed_dataset", task_ids="read_transform_staging")
        
        if not dataset:
            raise AirflowFailException("âŒ Aucun dataset trouvÃ© pour la sauvegarde")
        
        print(f"ğŸ“¥ Dataset reÃ§u avec {dataset['total_videos']} vidÃ©os")
        
        # Sauvegarder dans core avec transformation
        records_saved = transform_and_upsert_core(dataset)
        
        print(f"âœ… Sauvegarde rÃ©ussie: {records_saved} enregistrements upsertÃ©s dans core")
        
        # VÃ©rifier la sauvegarde
        core_collection = get_collection(config.CORE_COLLECTION)
        core_count = core_collection.count_documents({})
        
        print(f"ğŸ“Š Collection core maintenant: {core_count} enregistrements au total")
        
        # Statistiques finales
        save_result = {
            "records_processed": dataset['total_videos'],
            "records_saved": records_saved,
            "total_core_records": core_count,
            "status": "SUCCESS",
            "timestamp": datetime.utcnow().isoformat()
        }
        
        print(f"ğŸ‰ SAUVEGARDE TERMINÃ‰E AVEC SUCCÃˆS")
        print(f"   - Enregistrements traitÃ©s: {save_result['records_processed']}")
        print(f"   - Enregistrements sauvegardÃ©s: {save_result['records_saved']}")
        print(f"   - Total en base core: {save_result['total_core_records']}")
        
        return save_result
        
    except AirflowFailException:
        raise
    except Exception as e:
        error_msg = f"Erreur lors de la sauvegarde: {str(e)}"
        print(f"âŒ {error_msg}")
        raise AirflowFailException(error_msg)


def generate_pipeline_report(**context) -> Dict[str, Any]:
    """
    GÃ©nÃ©rer un rapport final du pipeline avec toutes les statistiques.
    
    Returns:
        Dict contenant le rapport complet
    """
    print("ğŸ“‹ === GÃ‰NÃ‰RATION DU RAPPORT FINAL ===")
    
    try:
        # RÃ©cupÃ©rer les rÃ©sultats de toutes les tÃ¢ches
        validation_result = context["ti"].xcom_pull(key="validation_result", task_ids="validate_quality")
        transform_result = context["ti"].xcom_pull(task_ids="read_transform_staging")
        save_result = context["ti"].xcom_pull(task_ids="save_to_core")
        
        # GÃ©nÃ©rer le rapport final
        pipeline_report = {
            "pipeline_name": "data_validation_pipeline",
            "execution_date": context["ds"],
            "timestamp": datetime.utcnow().isoformat(),
            "status": "SUCCESS",
            "validation": validation_result or {"status": "ERROR"},
            "transformation": transform_result or {"status": "ERROR"},
            "save_operation": save_result or {"status": "ERROR"},
            "summary": {
                "total_records_validated": validation_result.get("total_records", 0) if validation_result else 0,
                "total_records_saved": save_result.get("records_saved", 0) if save_result else 0,
                "pipeline_duration_seconds": (datetime.utcnow() - datetime.fromisoformat(context["ts"])).total_seconds()
            }
        }
        
        print("ğŸ“Š === RAPPORT FINAL DU PIPELINE ===")
        print(f"   ğŸ¯ Statut global: {pipeline_report['status']}")
        print(f"   ğŸ“ˆ Enregistrements validÃ©s: {pipeline_report['summary']['total_records_validated']}")
        print(f"   ğŸ’¾ Enregistrements sauvegardÃ©s: {pipeline_report['summary']['total_records_saved']}")
        print(f"   â±ï¸  DurÃ©e d'exÃ©cution: {pipeline_report['summary']['pipeline_duration_seconds']:.2f}s")
        print(f"   ğŸ“… Timestamp: {pipeline_report['timestamp']}")
        
        return pipeline_report
        
    except Exception as e:
        error_msg = f"Erreur lors de la gÃ©nÃ©ration du rapport: {str(e)}"
        print(f"âŒ {error_msg}")
        return {"status": "ERROR", "error": error_msg}


def send_alert_on_failure(**context) -> None:
    """
    Envoyer une alerte en cas d'Ã©chec du pipeline.
    """
    print("ğŸš¨ === ALERTE: Ã‰CHEC DU PIPELINE ===")
    
    try:
        # RÃ©cupÃ©rer les informations d'erreur
        task_instance = context["task_instance"]
        dag_run = context["dag_run"]
        
        alert_info = {
            "pipeline": "data_validation_pipeline",
            "status": "FAILED",
            "failed_task": task_instance.task_id,
            "execution_date": context["ds"],
            "timestamp": datetime.utcnow().isoformat(),
            "dag_run_id": dag_run.run_id
        }
        
        print(f"ğŸš¨ ALERTE GÃ‰NÃ‰RÃ‰E:")
        print(f"   - Pipeline: {alert_info['pipeline']}")
        print(f"   - TÃ¢che Ã©chouÃ©e: {alert_info['failed_task']}")
        print(f"   - Date d'exÃ©cution: {alert_info['execution_date']}")
        print(f"   - ID de run: {alert_info['dag_run_id']}")
        
        # Ici on pourrait ajouter l'envoi d'email, Slack, etc.
        # Pour l'instant, on log l'alerte
        print("ğŸ“§ Alerte loggÃ©e (intÃ©gration email/Slack Ã  implÃ©menter)")
        
    except Exception as e:
        print(f"âŒ Erreur lors de l'envoi d'alerte: {str(e)}")


# DÃ©finition du DAG
with DAG(
    dag_id="data_validation_pipeline",
    description="Pipeline intÃ©grÃ©: validation qualitÃ© â†’ transformation â†’ sauvegarde core avec alertes",
    default_args=DEFAULT_ARGS,
    schedule_interval="@daily",
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=["youtube", "validation", "pipeline", "core"],
) as dag:

    # TÃ¢che 1: Validation de la qualitÃ© des donnÃ©es
    validate_task = PythonOperator(
        task_id="validate_quality",
        python_callable=validate_data_quality,
        provide_context=True,
    )

    # TÃ¢che 2: Lecture et transformation des donnÃ©es staging
    transform_task = PythonOperator(
        task_id="read_transform_staging",
        python_callable=read_and_transform_staging_data,
        provide_context=True,
    )

    # TÃ¢che 3: Sauvegarde dans core_data
    save_task = PythonOperator(
        task_id="save_to_core",
        python_callable=save_validated_data_to_core,
        provide_context=True,
    )

    # TÃ¢che 4: GÃ©nÃ©ration du rapport final
    report_task = PythonOperator(
        task_id="generate_report",
        python_callable=generate_pipeline_report,
        provide_context=True,
        trigger_rule="all_done",  # S'exÃ©cute mÃªme si les tÃ¢ches prÃ©cÃ©dentes Ã©chouent
    )

    # TÃ¢che 5: Alerte en cas d'Ã©chec
    alert_task = PythonOperator(
        task_id="send_failure_alert",
        python_callable=send_alert_on_failure,
        provide_context=True,
        trigger_rule="one_failed",  # S'exÃ©cute seulement si une tÃ¢che Ã©choue
    )

    # DÃ©finition des dÃ©pendances
    validate_task >> transform_task >> save_task >> report_task
    [validate_task, transform_task, save_task] >> alert_task
